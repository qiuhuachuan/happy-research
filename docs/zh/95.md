##### 十三项缺陷  
使用ChatGPT或搭载ChatGPT的新版必应时，一个核心问题在于其回答的随机性。在测试过程中，我们反复观察到模型生成的内容存在显著波动。例如，即使使用相同模型（如ChatGPT）或应用（如新版必应），在相同设置下以完全一致的提问方式输入，模型在不同对话会话中的回复内容与质量（如语境相关性和细节丰富度）仍会天差地别。为缓解此问题，用户可尝试在不同会话中重复提问并综合对比输出结果，或通过优化提问措辞来帮助模型更精准理解需求。清晰、无歧义、定义明确的请求更容易首次即获得高质量答案，且在不同会话中能保持较高一致性。用户需认识到，大语言模型与人类类似，对措辞高度敏感，但缺乏人脑的“模糊逻辑”能力，难以准确理解表达含混或结构不佳的请求。

另一个典型问题是ChatGPT及其衍生应用存在的“幻觉”现象。该术语特指当前大语言模型会以不可预测的方式虚构虚假信息、错误数据，或援引根本不存在的来源。我们近期在Han等人的论述中重点分析了这一现象。该问题在文献引用场景中尤为突出——模型常提供错误的参考文献信息，甚至编造不存在的文献来源。我们的实际体验也印证了这一点：ChatGPT与新版必应经常为科技论文生成错误的书目数据。值得注意的是，即使将新版必应模式从“创意模式”切换至“精确模式”，也未能有效改善此问题。用户在索要参考文献时，常会遇到错误的期刊名称、作者、出版年份或文章编号。这一点尤其令人意外，因为绝大多数科技论文的书目元数据均可免费公开获取，而ChatGPT等大语言模型正是基于海量公开文本训练而成。总之，由于模型中“幻觉”现象的不可预测性，若用户未经人工校验直接采用其提供的数据、事实或文献信息，将面临重大风险。

用户必须警惕这一陷阱，因为ChatGPT的语言往往经过精心打磨，行文风格酷似训练有素的科学家或专业语言编辑的笔触。其礼貌且笃定的语气更强化了回答的权威感——毕竟模型是基于人类创作的数十亿文本训练的。但实际上，这些听起来合情合理的答案可能完全错误。而验证过程却极为耗时费力，因为事实核查需要投入大量时间且常需极其细致的工作。