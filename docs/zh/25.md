此外，稿件和作者的相关保密信息也可能包含在其中。出版机构对在同行评审过程中使用生成式人工智能或AI辅助技术进一步表达了担忧，并提出以下几点理由：首先，只有人类审稿人才能对审稿报告的内容承担全部责任。其次，同行评审所需的关键性思维和原创性评估超出了这类技术（包括生成式人工智能和AI辅助技术）的能力范围。第三，AI工具有可能生成不正确、不完整或存在偏见的结论。读者可在"出版伦理"章节中"审稿人职责"部分，查阅题为"期刊同行评审中生成式人工智能与AI辅助技术的使用"的全文（Elsevier 2024c）。一些学术出版机构（如Taylor & Francis）也提出了类似要求（Taylor & Francis 2024；Cambridge University Press 2023）。根据现行指南，同行评审人员不应"将未发表稿件中的文件、图像或信息上传至无法保证保密性、公众可访问及可能自行存储或使用这些信息的数据库或工具"，包括ChatGPT等生成式AI工具（Taylor & Francis 2024）。

出版机构在保密性、专有权利和数据隐私权方面的考量是合理的，正因如此，像施普林格·自然在其指南中所述，为审稿人提供更安全的AI工具访问权限显得尤为迫切。关于第二点质疑，这或许成立。但诸如ChatGPT及其衍生应用（如新版Bing）等生成式AI，已在该任务领域展现出显著能力（参见表8至12），观察其在技术迭代中的演进将颇具意义。值得注意的是，第三点质疑同样如此——模型在评估科学内容时应当能够持续优化并生成更完善的回应。至于第一点，无论审稿人是否使用生成式AI辅助评审，最终对评审报告内容负责的都应是审稿人本人。归根结底，审稿人必须对其提交的内容全权负责，根据出版政策，这可能包括核查AI生成意见、进行事实核验，并在独立评估后自主判断AI的辅助建议是否适用于评审流程。

从期刊编辑的视角来看，我们通常能辨识出整篇或大部分审稿意见是否由AI生成。判断依据之一，是观察这些评语是否虽然相关但流于泛泛而谈，缺乏对稿件具体问题的针对性分析。例如在评估研究论文的缺陷与局限时，ChatGPT往往将话题扩散至核心内容之外的领域。而人类专家则会聚焦于稿件的具体问题：诸如研究方法、质量保证与控制、结果呈现与解读、文章结构、写作表述、语言拼写等细节，同时还会对研究的科学性、重要性与创新性进行整体评估。与AI面对人类挑战时惯用的那种措辞考究、积极建议的范式不同，人类专家通常会针对稿件的具体问题给出精准犀利的专业意见，其语言反而可能不那么精雕细琢。